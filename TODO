DONE

	Hook up the ip2location to the new server 5 backend via handler -> state

	Disable google pubsub by default. Enable with an env var.

	Disable google bigquery by default. Enable with an env var.

	Got the raspberry pi server initing with the server backend 5 in dev.

	Got the raspberry pi client connecting to the raspberry pi server and upgrading.

	Session data bytes was desynced with raspberry. Was 500 bytes, but now 1024.

TODO
	
	It seems now that the magic backend needs to be fixed, perhaps with master/leader system, and maybe a sequence number increased each time the master changes it. 

	In change of master, bump it by +1000

	Once this is done, add debug in session update, so I can see how many near relays are routable. Are there any near relays coming up for my client?

	I don't know... the relays, or the route matrix could be causing the no routes.

	Which is it?

























	-----------------

	Implement a new portal cruncher based around the service.go

	-----------------

	Extend server backend to write session update data messages to redis pubsub.

	-----------------

	Run the portal cruncher in the happy path and verify it gets the redis pubsub messages.

	-----------------

	Update the old database code to stash the "Latitude" and "Longitude" for datacenters in the parent struct, so it is compatible with the new database code.

	-----------------

	Implement func tests for each major service, especially around leader election and transfer in-situ

	-----------------

	We need to check the crypto signature on relay updates.

	-----------------












































    -----------------

    After this, it is time to plumb session update messages to analytics, then process them to perform insertion into bigquery.

    -----------------

    Once this is done, we basically have a working SDK5 backend in dev.

    -----------------

    The next step then is to setup a build process for relays in semaphore.

    -----------------

    Then extend the relay to have the concept of a "secondary" backend, which it will send relay updates to, but it will not shut down if it can't communicate with.

    -----------------

    Once this is done we can create a new prod5 environment, and set up a few relays in prod so that they use prod5 as secondary relay backend.

    -----------------

    Once this is verified stable, upgrade all relays so they report to the secondary prod5 relay backend.

    -----------------


































    -----------------

	analytics service is spamming this in dev:

		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping

	Why can't it read the cost_matrix_stats message?

	-----------------

	analytics is failing to insert into bigquery in dev:

		Jan 04 04:29:18 analytics-mig-hkq3 app[4833]: error: failed to publish bigquery entry: 244 row insertions failed (insertion of row [insertID: "9blW809c4pZF2Tre0gkTFV5UKaI"; insertIndex: 0] failed with error: {Location: "numRelays"; Message: "no such field: numRelays."; Reason: "invalid"}, insertion of row [insertID: "IFxlFo9QjZskGR38a35pHJAoItl"; insertIndex: 1] failed with error: {Location: "numDatacenters"; Message: "no such field: numDatacenters."; Reason: "invalid"}, insertion of row [insertID: "27d1wczpzBolHFsuiwfOsUdax04"; insertIndex: 2] failed with error: {Location: "numDatacenters"; Message: "no such field: numDatacenters."; Reason: "invalid"}, ...)
		Jan 04 04:29:18 analytics-mig-hkq3 app[4833]: error: failed to publish bigquery entry: 245 row insertions failed (insertion of row [insertID: "fqcpRXzcepXDie22eUl57AyUZV8"; insertIndex: 0] failed with error: {Location: "bytes"; Message: "no such field: bytes."; Reason: "invalid"}, insertion of row [insertID: "ce23BHskKZVXVQ8U0mlDfROp5Re"; insertIndex: 1] failed with error: {Location: "numRelays"; Message: "no such field: numRelays."; Reason: "invalid"}, insertion of row [insertID: "WH8doSUiENSPxzdIxjMnA04Jq6U"; insertIndex: 2] failed with error: {Location: "numRelays"; Message: "no such field: numRelays."; Reason: "invalid"}, ...)

	-----------------
